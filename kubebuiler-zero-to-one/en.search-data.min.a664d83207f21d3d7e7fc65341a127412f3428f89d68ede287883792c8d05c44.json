[{"id":0,"href":"/kubebuilder-zero-to-one/01-kubebuilder-init-project/","title":"01 Kubebuilder Init Project","section":"","content":"使用 kuberbuilder 初始化项目 #  kubebuilder init --domain tangx.in kubebuilder create api --group myapp --version v1 --kind Redis apiVersion: myapp.tangx.in/v1 kind: Redis metadata: name: my-op-redis spec: replicas: 1 port: 3333 # 安装 make install # 卸载 make uninstall 查看 crd\nk get crd |grep tangx.in redis.myapp.tangx.in 2021-11-19T06:16:43Z "},{"id":1,"href":"/kubebuilder-zero-to-one/02-simplest-redis-crd/","title":"02 Simplest Redis Crd","section":"","content":"简单跑一跑 #  定义 CRD Redis 对象字段 #  在 /api/v1/redis_types.go 中， 增加 Replicas 和 Port 字段。\ntype RedisSpec struct { // INSERT ADDITIONAL SPEC FIELDS - desired state of cluster \t// Important: Run \u0026#34;make\u0026#34; to regenerate code after modifying this file  // Foo is an example field of Redis. Edit redis_types.go to remove/update \t// Foo string `json:\u0026#34;foo,omitempty\u0026#34;` \tReplicas int `json:\u0026#34;replicas,omitempty\u0026#34;` Port int32 `json:\u0026#34;port,omitempty\u0026#34;` } 这个 RedisSpec 对应 /deploy/my-op-redis.yml 中的 spec\napiVersion: myapp.tangx.in/v1 kind: Redis metadata: name: my-op-redis spec: replicas: 1 port: 3333 编码 Reconcile 调谐逻辑 #  在 /controllers/redis_controller.go 中编码 Reconcile(调谐) 逻辑。\nfunc (r *RedisReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) { _ = log.FromContext(ctx) // TODO(user): your logic here  redis := v1.Redis{} err := r.Get(ctx, req.NamespacedName, \u0026amp;redis) if err != nil { return ctrl.Result{}, err } fmt.Println(\u0026#34;得到crd redis 对象: \u0026#34;, redis) return ctrl.Result{}, nil } 启动调试 #  make uninstall make install make run 新开窗口\nka -f deploy/ 在 make run 窗口可以看到 调谐 中的输出结果。\n得到crd redis 对象: { {Redis myapp.tangx.in/v1} {my-op-redis default c0e85341-edf3-4261-92da-6a337d473f0c 775203 1 2021-11-19 15:26:53 +0800 CST \u0026lt;nil\u0026gt; \u0026lt;nil\u0026gt; map[] map[kubectl.kubernetes.io/last-applied-configuration:{\u0026quot;apiVersion\u0026quot;:\u0026quot;myapp.tangx.in/v1\u0026quot;,\u0026quot;kind\u0026quot;:\u0026quot;Redis\u0026quot;,\u0026quot;metadata\u0026quot;:{\u0026quot;annotations\u0026quot;:{},\u0026quot;name\u0026quot;:\u0026quot;my-op-redis\u0026quot;,\u0026quot;namespace\u0026quot;:\u0026quot;default\u0026quot;},\u0026quot;spec\u0026quot;:{\u0026quot;port\u0026quot;:3333,\u0026quot;replicas\u0026quot;:1}} ] [] [] [{kubectl-client-side-apply Update myapp.tangx.in/v1 2021-11-19 15:26:53 +0800 CST FieldsV1 {\u0026quot;f:metadata\u0026quot;:{\u0026quot;f:annotations\u0026quot;:{\u0026quot;.\u0026quot;:{},\u0026quot;f:kubectl.kubernetes.io/last-applied-configuration\u0026quot;:{}}},\u0026quot;f:spec\u0026quot;:{\u0026quot;.\u0026quot;:{},\u0026quot;f:port\u0026quot;:{},\u0026quot;f:replicas\u0026quot;:{}}} }]} {1 3333} {} } "},{"id":2,"href":"/kubebuilder-zero-to-one/03-deploy-crd-controller/","title":"03 Deploy Crd Controller","section":"","content":"发布 crd controller #    设置 docker server 网络代理， 避免编译的时候下载所依赖的 gcr.io 镜像失败。 参考文章 设置 docker server 网路代理\n  修改 Makefile, 设置默认 image name\n  VERSION ?= v$(shell cat .version) # Image URL to use all building/pushing image targets IMG ?= cr.docker.tangx.in/jtredis/controller:$(VERSION) 修改镜像 pull 策略。 在 /config/manager/manager.yaml 配置文件中， 添加 imagePullPolicy 策略。 由于本地开发， 并不准备上传到云上， 所以设置为 IfNotPresent。  spec: securityContext: runAsNonRoot: true containers: - command: - /manager args: - --leader-elect image: controller:latest name: manager ## 由于不上传到镜像仓库， 所以这里以本地编译的版本为准 imagePullPolicy: IfNotPresent 执行编译  make docker-build 发布  make deploy "},{"id":3,"href":"/kubebuilder-zero-to-one/04-filed-validation-by-comment/","title":"04 Filed Validation by Comment","section":"","content":"使用注解完整字段值约束 #  在 /api/v1/redis_types.go 中，使用注解完成字段值约束。\n 约束条件必须以 //+kubebuilder:validation:\u0026lt;METHOD\u0026gt;:=\u0026lt;VALUE\u0026gt; 为格式， 符号之间 没有空格。 约束条件必须 紧邻 字段， 且在字段上方。   https://book.kubebuilder.io/reference/markers/crd-validation.html\n type RedisSpec struct { // INSERT ADDITIONAL SPEC FIELDS - desired state of cluster \t// Important: Run \u0026#34;make\u0026#34; to regenerate code after modifying this file  Replicas int `json:\u0026#34;replicas,omitempty\u0026#34;` //+kubebuilder:validation:Minimum:=1234 \t//+kubebuilder:validation:Maximum:=54321 \tPort int32 `json:\u0026#34;port,omitempty\u0026#34;` } 重新编译安装\nmake install 使用命令查看查看\nkg crd redis.myapp.tangx.in Spec: Description: RedisSpec defines the desired state of Redis Properties: Port: Format: int32 Maximum: 54321 Minimum: 1234 Type: integer Replicas: Type: integer Type: object 将 /deploy/my-op-redis.yml 的端口改为 333 测试\napiVersion: myapp.tangx.in/v1 kind: Redis metadata: name: my-op-redis spec: replicas: 1 port: 333 部署， 并得到报错信息。\nka -f deploy The Redis \u0026quot;my-op-redis\u0026quot; is invalid: spec.port: Invalid value: 333: spec.port in body should be greater than or equal to 1234 "},{"id":4,"href":"/kubebuilder-zero-to-one/05-filed-validation-by-webhook/","title":"05 Filed Validation by Webhook","section":"","content":"通过 webhook 进行字段验证 #  通过 kubebuilder 生成代码 #  # 创建 api kubebuilder create api --group myapp --version v1 --kind Redis # 创建 api 的 webhook kubebuilder create webhook --group myapp --version v1 --kind Redis --defaulting --programmatic-validation 增加 webhook 条件 #  在 /api/v1/redis_webhook.go 中增加检查条件。\n检查 webhook 被触发有三个条件 Create / Update / Delete 时间节点, 分别对应三个方法。\n如下是 创建时检查\nfunc (r *Redis) ValidateCreate() error { redislog.Info(\u0026#34;validate create\u0026#34;, \u0026#34;name\u0026#34;, r.Name) // 条件判断 \tif r.ObjectMeta.Name == \u0026#34;tangx-in\u0026#34; { return fmt.Errorf(\u0026#34;不合法名字: tangx-in\u0026#34;) } if r.Spec.Port \u0026lt; 6379 { return fmt.Errorf(\u0026#34;端口必须大于等于 6379\u0026#34;) } // TODO(user): fill in your validation logic upon object creation. \treturn nil } 安装 cert-manager 管理证书 #  wget -c https://github.com/jetstack/cert-manager/releases/download/v1.6.1/cert-manager.yaml ka -f cert-manager.yaml 反注释 kustomize 渲染配置 #  /config/default/kustomization.yaml 中 ， 反注释遗下内容。\nbases: - ../webhook  # 引用 webhook 代码 - ../certmanager  # 引用 cert-manager 代码 ## 合并 patchesStrategicMerge: - manager_webhook_patch.yaml - webhookcainjection_patch.yaml # the following config is for teaching kustomize how to do var substitution vars: # [CERTMANAGER] To enable cert-manager, uncomment all sections with \u0026#39;CERTMANAGER\u0026#39; prefix. - name: CERTIFICATE_NAMESPACE # namespace of the certificate CR objref: kind: Certificate group: cert-manager.io version: v1 name: serving-cert # this name should match the one in certificate.yaml fieldref: fieldpath: metadata.namespace - name: CERTIFICATE_NAME objref: kind: Certificate group: cert-manager.io version: v1 name: serving-cert # this name should match the one in certificate.yaml - name: SERVICE_NAMESPACE # namespace of the service objref: kind: Service version: v1 name: webhook-service fieldref: fieldpath: metadata.namespace - name: SERVICE_NAME objref: kind: Service version: v1 name: webhook-service 编译安装 #   清理环境， 卸载之前的， 避免污染  make uninstall 编译带有 webhook 的新代码, 并发布  make docker-build make install make deploy 测试 #  apiVersion: myapp.tangx.in/v1 kind: Redis metadata: name: tangx-in spec: replicas: 1 port: 3333 分别执行两次带有不合法的参数\nka -f deploy/my-op-redis.yml Error from server (端口必须大于等于 6379): error when creating \u0026#34;deploy/my-op-redis.yml\u0026#34;: admission webhook \u0026#34;vredis.kb.io\u0026#34; denied the request: 端口必须大于等于 6379 ka -f deploy Error from server (不合法名字: tangx-in): error when creating \u0026#34;deploy/my-op-redis.yml\u0026#34;: admission webhook \u0026#34;vredis.kb.io\u0026#34; denied the request: 不合法名字: tangx-in 注意: 本地测试 #  在 /main.go 中， 注释以下内容忽略 webhook 部署， 方便调试\n// 本地测试可以注释 \tif err = (\u0026amp;myappv1.Redis{}).SetupWebhookWithManager(mgr); err != nil { setupLog.Error(err, \u0026#34;unable to create webhook\u0026#34;, \u0026#34;webhook\u0026#34;, \u0026#34;Redis\u0026#34;) os.Exit(1) } "},{"id":5,"href":"/kubebuilder-zero-to-one/06-create-pod-by-redis-operator/","title":"06 Create Pod by Redis Operator","section":"","content":"使用 Operator 创建并发布一个 Pod #  1. 组装 k8s api 创建 pod #  创建 /controllers/helper 目录， 这里面的代码实现 k8s Workloads 的创建。 具体实现就是封装 k8s workloads 的 api 对象\n// CreateRedis 创建 redis pod func CreateRedisPod2(ctx context.Context, client client.Client, config *appv1.Redis) error { pod := \u0026amp;corev1.Pod{} pod.Name = config.Name pod.Namespace = config.Namespace pod.Spec.Containers = []corev1.Container{ { Name: config.Name, Image: config.Spec.Image, ImagePullPolicy: corev1.PullIfNotPresent, Ports: []corev1.ContainerPort{ { ContainerPort: config.Spec.Port, }, }, }, } // ctx := context.Background() \treturn client.Create(ctx, pod) } 补充说明一下，为什么要把 helper 放在 /controllers 目录下。\n 其一， helper 是 controllers 的实现操作的行为， 算 controller 的一部分 其二， /Dockerfile 中在编译 go 代码的时候是有选择性的将代码目录复制进去的。 如果在根目录上自建目录( ex. /helper )， 那么就需要额外修改 Dockerfile，  # Copy the go sourceCOPY main.go main.goCOPY api/ api/COPY controllers/ controllers/## 添加COPY helper/ helper否则编译不通过，报错说找不到 helper 目录。\nStep 10/15 : RUN CGO_ENABLED=0 GOOS=linux GOARCH=64 go build -a -o manager main.go ---\u0026gt; Running in 66ae51d64eca controllers/redis_controller.go:30:2: no required module provides package github.com/tangx/k8s-operator-demo/helper; to add it: go get github.com/tangx/k8s-operator-demo/helper The command \u0026#39;/bin/sh -c CGO_ENABLED=0 GOOS=linux G 2. 添加主机并使用 rbac 授权 Operator 操作 k8s #  上面实现了创建 k8s pod 的 api 之后， 将 operator 编译并发不到 k8s 集群中。\n# 编译并发布 make docker-build make install make deploy ## 创建 operator 实例 ka -f deploy/ 在创建 redis 实例的时候， operator controller pod 报错如下， serviceaccount 没有权限操作 pods。\n2021-11-20T09:10:59.042Z\tERROR\tcontroller.redis\tReconciler error\t{\u0026quot;reconciler group\u0026quot;: \u0026quot;myapp.tangx.in\u0026quot;, \u0026quot;reconciler kind\u0026quot;: \u0026quot;Redis\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;my-op-redis\u0026quot;, \u0026quot;namespace\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;error\u0026quot;: \u0026quot;创建 redis pod 失败: pods is forbidden: User \\\u0026quot;system:serviceaccount:k8s-operator-demo-system:k8s-operator-demo-controller-manager\\\u0026quot; cannot create resource \\\u0026quot;pods\\\u0026quot; in API group \\\u0026quot;\\\u0026quot; in the namespace \\\u0026quot;default\\\u0026quot;\u0026quot;} 这时因为生成的 /config/rbac/role.yaml 文件中的 rbac 权限不够。\n在 /controllers/redis_controller.go 的 Reconcile 方法上方，添加注解\n格式如下\n//+kubebuilder:rbac:groups=\u0026#34;资源组名称\u0026#34;,resources=资源名称,verbs=get;list;watch;create;update;patch;delete（操作动作） 为了要对 pod 具有操作权限， 需要对 增加对应的 kubebuilder 注解。\n//+kubebuilder:rbac:groups=\u0026#34;\u0026#34;,resources=pods,verbs=get;list;watch;create;update;patch;delete  func (r *RedisReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) { // statement } 这样我们实现的 operator 就可以对 pod 进行增删改等操作了。\n如果不知道 资源组名称 和 资源名称， 可以使用命令 kubectl api-resources 查看\nkubectl api-resources NAME SHORTNAMES APIVERSION NAMESPACED KIND pods po v1 true Pod deployments deploy apps/v1 true Deployment  groups: 对应的 NAME 字段就是 资源名称 rexources: 对应的 APIVERSION 字段， 去掉 /版本号 就是 资源组名称  因此 pods 隶属于 k8s core 组。 资源组名称 就是空 \u0026quot;\u0026quot; deployments 隶属于 k8s apps 组， 资源组名称 就是 apps   verbs: 对应的就是操作动作。  同样在 yaml 文件也可以快速看出隶属于什么 apis\n# kgd deployment-name -o yaml apiVersion: apps/v1 kind: Deployment # kgp -o yaml pod-name apiVersion: v1 kind: Pod  https://kubernetes.io/zh/docs/reference/access-authn-authz/rbac/\n 3. 注释 webhook 方便测试 #  为了方便本地测试， 对 /main.go 中的 SetupWebhookWithManager webhook 相关代码增加了一个条件判断。\n如下\n// 本地测试可以注释 \tif env := os.Getenv(\u0026#34;ENV\u0026#34;); env != \u0026#34;local\u0026#34; { if err = (\u0026amp;myappv1.Redis{}).SetupWebhookWithManager(mgr); err != nil { setupLog.Error(err, \u0026#34;unable to create webhook\u0026#34;, \u0026#34;webhook\u0026#34;, \u0026#34;Redis\u0026#34;) os.Exit(1) } } 同时在 Makefile 中也增加相应的环境变量注入\n.PHONY: run run: manifests generate fmt vet ## Run a controller from your host. \tENV=local go run ./main.go 这样就不用在 测试和编译 之间来回注释/反注释这段代码了。\n"},{"id":6,"href":"/kubebuilder-zero-to-one/07-1-delete-pod-by-redis-ownerreference/","title":"07 1 Delete Pod by Redis Owner Reference","section":"","content":"使用 OwnerReference 管理 redis operator 创建的 Pod #   https://kubernetes.io/blog/2021/05/14/using-finalizers-to-control-deletion/\n 在上一章的代码可以通过如下命令创建一个 redis 实例， 并随即创建一个 Pod\nka -f deploy/ 但是在使用如下命令删除 redis 实例时， 虽然命令行界面提示删除成功， 但是创建的 Pod 依旧存在。\nkrm -f deploy/ 其原因是 redis 实例 与 Pod 之间 没有 建立关联关系。\n那要如何创建关联关系呢？ 可以参考阅读官方博客， 使用 finalizer 控制删除。\n在 Owner Reference 一节中提到了资源的 父子关系 。\n根据这个原理， 更新 /controllers/helper/redis_helper.go 的 Pod 创建 API， 加入 OwnerReference 相关的代码。\n// CreateRedis 创建 redis pod func CreateRedisPod(ctx context.Context, client client.Client, config *appv1.Redis) error { pod := \u0026amp;corev1.Pod{ ObjectMeta: metav1.ObjectMeta{ Name: fmt.Sprintf(\u0026#34;%s-%d\u0026#34;, config.Name, time.Now().Unix()), Namespace: config.Namespace, // 创建 OwnerReference \tOwnerReferences: []metav1.OwnerReference{ ownerReference(*config), }, }, // 省略 \t} } func ownerReference(config appv1.Redis) metav1.OwnerReference { return metav1.OwnerReference{ APIVersion: config.APIVersion, Kind: config.Kind, Name: config.Name, UID: config.UID, Controller: ptrBool(true), BlockOwnerDeletion: ptrBool(true), } } 编译测试 #  重新编译，并部署安装\nmake docker-build make install make deploy #### 创建 # ka -f deploy redis.myapp.tangx.in/my-op-redis created # kgp NAME READY STATUS RESTARTS AGE my-op-redis-1637576923 0/1 Running 0 1s #### 删除 # krm -f deploy/ redis.myapp.tangx.in \u0026#34;my-op-redis\u0026#34; deleted # kgp NAME READY STATUS RESTARTS AGE my-op-redis-1637576923 1/1 Terminating 0 10s "},{"id":7,"href":"/kubebuilder-zero-to-one/07-2-delete-pod-by-finalizers/","title":"07 2 Delete Pod by Finalizers","section":"","content":"使用 finalizers 管理 redis operator 创建的 Pod #   https://kubernetes.io/blog/2021/05/14/using-finalizers-to-control-deletion/\n 上一章使用了 OwnerReference 关联 redis instance 和所创建的 Pod， 这里的删除是通过 k8s 内置的关系处理器处理的。\n根据官方博客文档中的阐述， 当一个资源的额 finalizers 没有被清空时， 这个资源将无法被删除。 因此， 本章通过 finalizers\n 来建立 redis instance 和所创建 pod 的关系， 以及处理删除逻辑  1. 创建 redis instance 与 pod 的关系 #  在 /controllers/helper/redis_helper.go 通过 Finalizers 管理 Pod\n// CreateRedis 创建 redis pod func CreateRedisPod2(ctx context.Context, client client.Client, redis *appv1.Redis) error { isUpdated := false for i := 0; i \u0026lt; redis.Spec.Replicas; i++ { name := fmt.Sprintf(\u0026#34;%s-%d\u0026#34;, redis.Name, i) fmt.Println(\u0026#34;创建 pod lo :\u0026#34;, name) pod := getPod2(redis, name) if isPodExist2(redis, pod.Name) { continue } if err := client.Create(ctx, pod); err != nil { return err } // 使用 Finalizer 管理创建的 Pod。 当 pod 被删除完的时候，才能删除 redis \tredis.Finalizers = append(redis.Finalizers, pod.Name) isUpdated = true } // redis.Finalizers 的变更是在本地内存中， 使用 update 更新到 k8s 中 \tif isUpdated { return client.Update(ctx, redis) } return nil }  所有通过 redis instance 创建新创建的 Pod 使用 pod.Name 为 key ， 保存在 redis.Finalizers 中进行管理。  redis.Finalizers = append(redis.Finalizers, pod.Name) 为了保证只有出现 pod 变更的时候才进行 redis 的 update 操作（幂等）， 使用了 isUpdated 作为信号条件。  // redis.Finalizers 的变更是在本地内存中， 使用 update 更新到 k8s 中 \tif isUpdated { return client.Update(ctx, redis) } 注意: append() 操作虽然将 pod.Name 加入到了 redis.Finalizers 中， 单这是在本地内存实现的。 因此必须要使用 client.Update() 操作将变更保存到 k8s 中\n2. 删除 redis instance 与 pod #  根据博客中指出:\n 在触发删除的时候， redis instance 会多一个 DeletionTimestamp 的标识， 具有该标识的实例 1. 处于制度状态， 但 2. 能管理操作其 finalizers 字段 在 redis.Finalizers 不为空的时候， redis instance 是处于 删除状态 被夯住的。  因此如果要进行删除逻辑， 则需要先进行 标识判断\n在 /controllers/redis_controller.go 中\nfunc (r *RedisReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) { // 省略 \t// 删除 逻辑 \t// IsZero 标识这个字段为 nil 或者 零值， 即非删除状态 \t// 删除状态则 取反 \tif !redis.DeletionTimestamp.IsZero() { err = helper.DeleteRedis2(ctx, r.Client, \u0026amp;redis) if err != nil { return ctrl.Result{}, fmt.Errorf(\u0026#34;删除 redis 失败:%v\u0026#34;, err) } return ctrl.Result{}, nil } // 省略 } 删除逻辑代码如下\nfunc DeleteRedis2(ctx context.Context, client client.Client, redis *appv1.Redis) error { fmt.Println(\u0026#34;进入删除循环咯\u0026#34;) isUpdated := false for _, name := range redis.Finalizers { pod := getPod2(redis, name) if err := client.Delete(ctx, pod); err != nil { return fmt.Errorf(\u0026#34;删除 pod (%s) 失败: %v\\n\u0026#34;, name, err) } deleteFromFinalizers(redis, pod.Name) isUpdated = true } if isUpdated { if err := client.Update(ctx, redis); err != nil { return fmt.Errorf(\u0026#34;更新 redis 失败: %v\\n\u0026#34;, err) } return client.Delete(ctx, redis) } return nil }  进入到删除逻辑后, 遍历 redis.Finalizers 获取所有被管理的 pod.Name， 依次删除 删除成功后， 将 pod.Name 从 redis.Finalizers 中删除 当本地 redis.Finalizers 被清空后， 将状态更新到 k8s 中。 至此 k8s 回收功能就可以清理 redis instance 了。  3. 退出调谐 Reconcile #  当 Reconcile 错误退出的时候( err!=nil ), k8s 认为资源状态没有达到预期， 调谐会不断的进行重试。 即 Reconcile 会不断的执行。\nfunc (r *RedisReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) { fmt.Println(\u0026#34;进入 redis Reconcile, 检查调谐状态\u0026#34;) defer fmt.Println(\u0026#34;退出 redis Reconcile 调谐状态\u0026#34;) _ = log.FromContext(ctx) // TODO(user): your logic here  redis := v1.Redis{} err := r.Get(ctx, req.NamespacedName, \u0026amp;redis) if err != nil { // 如果 err !=nil , k8s 调谐会不断重试。 因此找不到资源， 则直接返回 err=nil \t// return ctrl.Result{}, fmt.Errorf(\u0026#34;Reconcile 获取 redis 失败: %v\u0026#34;, err)  // 找不到返回 nil，成功处理， 退出循环。 \treturn ctrl.Result{}, nil } } 因此如果资源找不到则直接返回 err==nil, 表示资源状态达到 预期， 停止调谐。\n"},{"id":8,"href":"/kubebuilder-zero-to-one/08-scale-pod/","title":"08 Scale Pod","section":"","content":"Pod 扩容与缩容 #  代码分支越来越多 增/删/改 都有了， 于是选择拆分为 3 个分支。\n// 扩容 func (r *RedisReconciler) increaseReconcile(ctx context.Context, redis *myappv1.Redis) (ctrl.Result, error) { // ... } // 缩容 func (r *RedisReconciler) decreaseReconcile(ctx context.Context, redis *myappv1.Redis) (ctrl.Result, error) { // ... } // 删除 func (r *RedisReconciler) deleteReconcile(ctx context.Context, redis *myappv1.Redis) (ctrl.Result, error) { // ... } 所谓 扩容/缩容， 在通过 finalizers 管理的时候就是 redis.spec.replicas 与 len(redis.finalizers) 的大小比较。\n// 缩容 \tif len(redis.Finalizers) \u0026gt; redis.Spec.Replicas { return r.decreaseReconcile(ctx, \u0026amp;redis) } 在实现过程中，保证行为幂等就行。\n缩容代码如下\nfunc DecreaseRedis2(ctx context.Context, client client.Client, redis *appv1.Redis) error { isUpdated := false for _, name := range redis.Finalizers[redis.Spec.Replicas:] { pod := getPod2(redis, name) if err := client.Delete(ctx, pod); err != nil { return err } deleteFromFinalizers(redis, name) isUpdated = true } if isUpdated { return client.Update(ctx, redis) } return nil } 缩容的实现就是从 redis.finalizers 中取出被管理的 pod 名字并删除。 由于我们创建 pod 的时候\n 使用了 id 编号作为名字的 suffix (ex. redis-0 / redis-1) 并且是按照顺序的 push 到 redis.finalizers 中的。  因此这里的缩容行为逻辑就更简单的一点， 最后后面的几个 POD 。\n// 删除最后面的几个 Pod \tfor _, name := range redis.Finalizers[redis.Spec.Replicas:] { pod := getPod2(redis, name) if err := client.Delete(ctx, pod); err != nil { return err } deleteFromFinalizers(redis, name) } "},{"id":9,"href":"/kubebuilder-zero-to-one/09-watch-k8s-event/","title":"09 Watch K8s Event","section":"","content":"监听 k8s 事件 #  之前的代码遗留了一个问题， 当手动通过命令删除 pod 时候， 不会出发 redis.Finalizers 的更新， 也不会重建被删除的 Pod， 实现效果并不好\nkubectl delete pod pod_name 1. 监听事件 #  在 /controllers/redis_controller.go 中生成了对象和方法监听 k8s 的事件。\nctrl 创建的 Builder 可以通过 链式 调用方式， 监听多个 k8s 对象的事件。\n// SetupWithManager sets up the controller with the Manager. func (r *RedisReconciler) SetupWithManager(mgr ctrl.Manager) error { return ctrl.NewControllerManagedBy(mgr). For(\u0026amp;myappv1.Redis{}). // 监听 pod 事件 \tWatches( \u0026amp;source.Kind{Type: \u0026amp;corev1.Pod{}}, handler.Funcs{DeleteFunc: r.podDeleteHandler}, ). Watches( \u0026amp;source.Kind{Type: \u0026amp;v1.Service{},}, handler.Funcs{DeleteFunc: r.podDeleteHandler}, ). Complete(r) } 上述代码中， 使用 Watches 方法监听了 Pod 和 Service 的事件。 Watch 的两个参数\n 监听事件对象( source.Source): 可以是 source.Kind、 source.Channel 或 source.Informer。 监听事件行为( handler.EventHandler ): 当事件对象发生满足条件时所触发的行为。  Create: 对应 CreateFunc ， 满足 创建 条件是触发 Update: 对应 UpdateFunc ， 满足 更新 条件是触发 Delete: 对应 DeleteFunc ， 满足 删除 条件是触发 Generic: 对应 GenericFunc ， 满足 未知 条件是触发    2. 事件方法 #  每个事件方法都支持 2个 参数， 事件 与 工作队列 。\n// Funcs implements EventHandler. type Funcs struct { // Create is called in response to an add event. Defaults to no-op. \t// RateLimitingInterface is used to enqueue reconcile.Requests. \tCreateFunc func(event.CreateEvent, workqueue.RateLimitingInterface) // Update is called in response to an update event. Defaults to no-op. \t// RateLimitingInterface is used to enqueue reconcile.Requests. \tUpdateFunc func(event.UpdateEvent, workqueue.RateLimitingInterface) // Delete is called in response to a delete event. Defaults to no-op. \t// RateLimitingInterface is used to enqueue reconcile.Requests. \tDeleteFunc func(event.DeleteEvent, workqueue.RateLimitingInterface) // GenericFunc is called in response to a generic event. Defaults to no-op. \t// RateLimitingInterface is used to enqueue reconcile.Requests. \tGenericFunc func(event.GenericEvent, workqueue.RateLimitingInterface) } 通过事件， 可以拿到对象应变化的对象。 有了这些对象， 就可以定制更多的行为。\nfunc (r *RedisReconciler) podDeleteHandler(e event.DeleteEvent, q workqueue.RateLimitingInterface) { pname := e.Object.GetName() pns := e.Object.GetNamespace() fmt.Printf(\u0026#34;Pod %s in NS %s 被删除\\n\u0026#34;, pname, pns) } 上述是拿到 Pod 删除事件后， 打印出相关日志信息。\nkrm -f deploy/ Pod my-op-redis-1 in NS k8s-operator-demo-system 被删除 Pod my-op-redis-0 in NS k8s-operator-demo-system 被删除 3. 监听的资源与权限 #  既然是监听 k8s 资源， 那就必须对所监听的资源有响应的权限。\n以下的日志报错就是因为没有提前授权 Service 的权限造成的。\nE1123 08:10:53.183975 1 reflector.go:138] pkg/mod/k8s.io/client-go@v0.22.1/tools/cache/reflector.go:167: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User \u0026quot;system:serviceaccount:k8s-operator-demo-system:k8s-operator-demo-controller-manager\u0026quot; cannot list resource \u0026quot;services\u0026quot; in API group \u0026quot;\u0026quot; at the cluster scope 解决方法和之前提到的一样， 通过 kubebuilder 的注解方法生成对应的 rbac 权限。\n//+kubebuilder:rbac:groups=\u0026#34;\u0026#34;,resources=services,verbs=get;list;watch;create;update;patch;delete "},{"id":10,"href":"/kubebuilder-zero-to-one/10-recreate-deleted-pod/","title":"10 Recreate Deleted Pod","section":"","content":"重建被删除的 Pod #  之前遗留了一个问题， 直接用命令行删除的 Pod 不能被重建。 这次就来解决它。\n首先来整理之前遗留的问题故障点在哪里？\n 使用命令 kubectl delete 直接删除 pod 的时候， redis.Finalizers 不会变更， 依旧包含被删除的 pod.Name。 在创建 Pod 的时候， 判断 Pod 是否存在使用的是 redis.Finalizers 提供信息， 而 没有判断 k8s 中真实的情况。 没有机制 通知 redis operator 进行检测或重建。  因此全新流程如下\n Pod 状态变化: kubectl delete 删除 Pod Redis 重新调谐: 通知 Redis operator 变化， 重新启动 调谐(Reconcile) 创建 Pod 的逻辑如下  如果 Pod 在 k8s 中存在， 则跳过。 （为了降低复杂性， 不考虑直接改变 redis.finalizers 的情况） 如果 Pod 不存在， 创建 Pod。 是否更新 redis.Finaliers， 取决于 Pod 是 新建 或者 重建。  新建 如果 pod.Name 不存在， 则 append 到末尾。 这点保持不变。 重建 如果 pod.Name 存在其中， 则跳过。      1. 通知 Redis Operator 变化 #  上一章中已经提到了， redis operator 可以订阅 k8s 事件， /controllers/redis_controller.go 代码如下\n代码中订阅了 Pod 事件。 当 Pod 发生 删除事件 后， 回调 r.podDeleteHandler 进行处理。\n// SetupWithManager sets up the controller with the Manager. func (r *RedisReconciler) SetupWithManager(mgr ctrl.Manager) error { return ctrl.NewControllerManagedBy(mgr). For(\u0026amp;myappv1.Redis{}). // 监听 pod 事件 \tWatches( \u0026amp;source.Kind{ Type: \u0026amp;corev1.Pod{}, }, handler.Funcs{ DeleteFunc: r.podDeleteHandler, }, ) Complete(r) } 在回调函数 r.podDeleteHanlder 中， 就需要实现通知的前置条件， 找到需要通知的 redis operator 对象。 至于如何通知， kubebuilder 已经给我们封装好了， 不用过多考虑。\n要实现 redis operator 的通知， 其 关键信息 在于 OwnerReferences。\n在 官方博文 - 使用 finalizers 控制删除行为 提到过 父子资源 之间可以通过 OwnerReference 进行关联形成关系树， 有利于资源的控制、跟踪、管理和回收。\n在这里可以简单的认为， redis operator 在创建 子 pod 的时候， 使用 OwnerReference 注入了一些自身相关的有效性信息。\n这部分的代码实现， 可以查看 07.1 使用 OwnerReference 关系删除 Pod 对象\nfunc (r *RedisReconciler) podDeleteHandler(e event.DeleteEvent, q workqueue.RateLimitingInterface) { ns := e.Object.GetNamespace() for _, owner := range e.Object.GetOwnerReferences() { // 非法 owner 不引起调谐 \tif owner.APIVersion != \u0026#34;myapp.tangx.in/v1\u0026#34; || owner.Kind != \u0026#34;Redis\u0026#34; { continue } // 入队， 通知 redis operator 变更， 进行重新 调谐。 \tq.Add( reconcile.Request{ NamespacedName: types.NamespacedName{ Namespace: ns, Name: owner.Name, }, }, ) } } 上述 handler 代码中，在删除事件 e 发生时\n 通过 e.Object.GetNamespace() 获取到被删除的 Pod 所在的 namespace。  redis operator 和 pod 是在同一个 namespace 下。   通过 e.Object.GetOwnerReferences() 获取到与 Pod 所有相关的 父资源 对象。 循环便利所有 Owners, 获得 owner 资源名称  将 owner 的 namespace 和 name 包装一下， 成为 reconcile.Request 对象 将新包装的对象加入到 q workqueue.RateLimitingInterface 队列中。   之后一切交给 k8s 完成。  不论处于性能还是安全考虑， 都应该增加如下代码。 非本 Operator 创建的 Pod 资源的生命周期行为应该被忽略。\n// 非法 owner 不引起调谐 if owner.APIVersion != \u0026#34;myapp.tangx.in/v1\u0026#34; || owner.Kind != \u0026#34;Redis\u0026#34; { continue } 否则任何 Pod 的删除都将引起 redis operator 的 Reconcile 行为。\n2. Pod 创建流程的变化 #  2.1 OwnerReference 支持 #  上一节已经提到了， 实现通知的前提是依赖 OwnerReference。\n在 /controllers/helper/redis_helper.go 创建 pod 对象明细的相关代码中加入相关代码。\nfunc getPod2(redis *appv1.Redis, name string) *corev1.Pod { pod := \u0026amp;corev1.Pod{} pod.Name = name pod.Namespace = redis.Namespace // 创建 pod 时添加 OwnerReference \tpod.ObjectMeta.OwnerReferences = []metav1.OwnerReference{ ownerReference(*redis), } // .. 省略 } func ownerReference(config appv1.Redis) metav1.OwnerReference { return metav1.OwnerReference{ APIVersion: config.APIVersion, Kind: config.Kind, Name: config.Name, UID: config.UID, Controller: ptrBool(true), BlockOwnerDeletion: ptrBool(true), } } 2.2 Pod 创建流程行为变更 #  // CreateRedis 创建 redis pod func CreateRedisPod2(ctx context.Context, client client.Client, redis *appv1.Redis) error { isUpdated := false for i := 0; i \u0026lt; redis.Spec.Replicas; i++ { name := fmt.Sprintf(\u0026#34;%s-%d\u0026#34;, redis.Name, i) fmt.Println(\u0026#34;创建 pod lo :\u0026#34;, name) // 如果在 k8s 中存在则跳过。 暂不考虑有人直接修改 redis 的 finalizers 的情况 \tif isPodExistInK8S(ctx, client, redis.Namespace, name) { continue } pod := getPod2(redis, name) if err := client.Create(ctx, pod); err != nil { return err } // 如果 pod.Name 在 finaliers 中， 则为删后重建。 \tif isPodExistInFinalizers2(redis, pod.Name) { continue } // 如果 pod.Name 不在 finalizers 中， 则为新增 pod。 \t// 使用 Finalizer 管理创建的 Pod。 当 pod 被删除完的时候，才能删除 redis \tredis.Finalizers = append(redis.Finalizers, pod.Name) isUpdated = true } // redis.Finalizers 的变更是在本地内存中， 使用 update 更新到 k8s 中 \tif isUpdated { return client.Update(ctx, redis) } return nil } // isPodExistInK8S 检测 pod 是否在 k8s 中存在 // true 为存在 func isPodExistInK8S(ctx context.Context, client client.Client, namespace string, name string) bool { key := types.NamespacedName{ Namespace: namespace, Name: name, } // 这里偷懒， 没有进行错误内容检测。 \terr := client.Get(ctx, key, \u0026amp;corev1.Pod{}) return err == nil } 之前提到了， 由于 kubectl delete 删除的原因， 导致了 redis.Finalizers 的数据失真。\n因此在创建 Pod 时，\n 首选需要通过 isPodExistInK8S 检查 Pod 是否存在于 k8s 中， 如果 Pod 已存在则忽略； 不存在则继续创建。 使用 client.Create 创建 Pod。 使用检查 pod.Name 是否存在于 redis.Finialers 中。 如果存在则表示 Pod 属于 删后重建， 不更新 redis.Finialers ； 如果不存在则表示为 新建， 需要更新 redis.Finialers  "},{"id":11,"href":"/kubebuilder-zero-to-one/11-official-package-optimize/","title":"11 Official Package Optimize","section":"","content":"使用 controllerutil 优化代码 #  在之前的代码中， 对于 OwnerReference 和 Finalizers 操作我们自己实现了一些方法。 其实这些操作官方已经封好成包了， 开箱即用。\n复制 /controllers/helper 保存为 /controllers/helper2。 前者保存手工代码， 后者保存优化代码。\n https://pkg.go.dev/sigs.k8s.io/controller-runtime/pkg/controller/controllerutil\n Finalizers 操作 #  之前\n// 添加 func appendFinalizers(redis *appv1.Redis, name string){ // 如果 pod.Name 不在 finalizers 中， 则为新增 pod。 \t// 使用 Finalizer 管理创建的 Pod。 当 pod 被删除完的时候，才能删除 redis \tredis.Finalizers = append(redis.Finalizers, name) } // 存在判断 func isPodExistInFinalizers2(redis *appv1.Redis, name string) bool { for _, rname := range redis.Finalizers { if rname == name { return true } } return false } // 删除 func deleteFromFinalizers(redis *appv1.Redis, name string) { for i, rname := range redis.Finalizers { if rname == name { redis.Finalizers = append(redis.Finalizers[:i], redis.Finalizers[i+1:]...) return } } } 之后\n// 添加 controllerutil.AddFinalizer(redis, name) // 存在判断 controllerutil.ContainsFinalizer(redis, pod.Name) // 删除 controllerutil.RemoveFinalizer(redis, pod.Name) OwnerReference 操作 #  之前\nfunc ownerReference(config appv1.Redis) metav1.OwnerReference { return metav1.OwnerReference{ APIVersion: config.APIVersion, Kind: config.Kind, Name: config.Name, UID: config.UID, Controller: ptrBool(true), BlockOwnerDeletion: ptrBool(true), } } func ptrBool(b bool) *bool { return \u0026amp;b } 之后\n// 创建 pod 时添加 OwnerReference controllerutil.SetOwnerReference(redis, pod, scheme) 其中 scheme 为 RedisReconciler 中的 Scheme 字段\n// RedisReconciler reconciles a Redis object type RedisReconciler struct { client.Client Scheme *runtime.Scheme } "},{"id":12,"href":"/kubebuilder-zero-to-one/12-add-event/","title":"12 Add Event","section":"","content":"增加 event 事件支持 #  k8s 官方 controller 都实现了 Events 消息信息， 如下\nkubectl describe deployment k8s-operator-demo-controller-manager Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ScalingReplicaSet 15m deployment-controller Scaled up replica set k8s-operator-demo-controller-manager-75cc59d8ff to 1 Normal ScalingReplicaSet 14m deployment-controller Scaled down replica set k8s-operator-demo-controller-manager-b9d9f7886 to 0 我们自定义的 Operator 同样可以实现。\noperator 支持 event #   在 /controllers/redis_controller.go 中定义 RedisReconcile 的时候， 添加 EventRecord 字段。  // RedisReconciler reconciles a Redis object type RedisReconciler struct { client.Client Scheme *runtime.Scheme // 添加事件 \tEventRecord record.EventRecorder } 在 /main.go 中， 创建 mgr 的时候使用 mgr.GetEventRecorderFor(\u0026quot;RedisOperator\u0026quot;) 初始化 EventRecorder。  其中 RedisOperator 的值可以任意定义， 在 Event 日志中为 FROM 字段的值。 通常使用控制器名字 OperatorName， 例如这里使用 RedisOperator。\nif err = (\u0026amp;controllers.RedisReconciler{ Client: mgr.GetClient(), Scheme: mgr.GetScheme(), // 添加事件记录名称 \tEventRecord: mgr.GetEventRecorderFor(\u0026#34;RedisOperator\u0026#34;), }).SetupWithManager(mgr); err != nil { setupLog.Error(err, \u0026#34;unable to create controller\u0026#34;, \u0026#34;controller\u0026#34;, \u0026#34;Redis\u0026#34;) os.Exit(1) } 在需要的地方添加事件日志输出  使用 r.EventRecord.Event() 方法记录事件日志\nfunc (r *RedisReconciler) increaseReconcile(ctx context.Context, redis *myappv1.Redis) (ctrl.Result, error) { // 添加事件日志 \tr.EventRecord.Event(redis, corev1.EventTypeNormal, \u0026#34;扩容\u0026#34;, fmt.Sprintf(\u0026#34;%s 副本数设置为 %d\u0026#34;, redis.Name, redis.Spec.Replicas), ) // 创建 逻辑 \terr := helper2.CreateRedisPod2(ctx, r.Client, redis, r.Scheme) if err != nil { return ctrl.Result{}, fmt.Errorf(\u0026#34;创建 redis pod 失败: %v\u0026#34;, err) } return ctrl.Result{}, nil } 其中， Event 需要一下几个字段\nEvent(object runtime.Object, eventtype, reason, message string)  object: 记录日志的对象。 也就是 kd redis \u0026lt;name\u0026gt; 中的 name。 eventtype: Type 字段， 事件类型， 可以是任意值。  k8s corev1 api 中提供了 2个 官方值: Normal 和 Warning    const ( // Information only and will not cause any problems \tEventTypeNormal string = \u0026#34;Normal\u0026#34; // These events are to warn that something might go wrong \tEventTypeWarning string = \u0026#34;Warning\u0026#34; ) reason: Reason 字段， 事件原因， 可以理解为事件分类/类型。 message: Message 字段， 事件消息， 详细描述。  测试 #  kd redis my-op-redis # ...省略 Spec: Image: redis:5-alpine Port: 8333 Replicas: 1 Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal 扩容 66s (x2 over 66s) RedisOperator redis 副本数设置为 3 Warning 缩容 49s RedisOperator redis 副本数设置为 1 Normal 扩容 38s (x5 over 72s) RedisOperator redis 副本数设置为 1 "},{"id":13,"href":"/kubebuilder-zero-to-one/13-add-status/","title":"13 Add Status","section":"","content":"添加 Status 状态字段 #  添加 kd 状态字段 #  在 /api/v1/redis_types.go 的 RedisStatus 中添加需要展示的字段。\n这里添加一个副本数量。\ntype RedisStatus struct { // INSERT ADDITIONAL STATUS FIELD - define observed state of cluster \t// Important: Run \u0026#34;make\u0026#34; to regenerate code after modifying this file \tReplicas int `json:\u0026#34;replicas\u0026#34;` } 偷懒， 没有在创建或删除 pod 时进行精细控制。 而是使用 defer 在 Reconcile 退出的时候进行一次最终的赋值管理。\nfunc (r *RedisReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) { fmt.Println(\u0026#34;进入 redis Reconcile, 检查调谐状态\u0026#34;) defer fmt.Println(\u0026#34;退出 redis Reconcile 调谐状态\u0026#34;) _ = log.FromContext(ctx) redis := \u0026amp;myappv1.Redis{} defer func() { // 状态赋值 \tredis.Status.Replicas = len(redis.Finalizers) // 状态更新，偷懒忽略错误判断 \t_ = r.Status().Update(ctx, redis) }() // ...省略 }  注意: 必须要使用 r.Status().Update() 方法更新， 否则不会展示 Status 字段。\n 编译发布， 并测试\n# kg redis my-op-redis -o yaml apiVersion: myapp.tangx.in/v1 kind: Redis metadata: finalizers: - my-op-redis-0 - my-op-redis-1 generation: 1 name: my-op-redis namespace: k8s-operator-demo-system spec: image: redis:5-alpine port: 8333 replicas: 2 status: replicas: 2 添加 kg 打印字段 #   https://book.kubebuilder.io/reference/generating-crd.html#additional-printer-columns\n 在 /api/v1/redis_types.go 中， 使用 +kubebuilder:printcolumn 添加需要展示的字段。\n//+kubebuilder:object:root=true //+kubebuilder:subresource:status //+kubebuilder:printcolumn:name=\u0026#34;Replicas\u0026#34;,type=integer,JSONPath=`.status.replicas` //+kubebuilder:printcolumn:name=\u0026#34;ImageName\u0026#34;,type=string,JSONPath=`.spec.image` //+kubebuilder:printcolumn:name=\u0026#34;Uuid\u0026#34;,type=string,JSONPath=`.metadata.uid` //+kubebuilder:printcolumn:name=\u0026#34;Alias\u0026#34;,type=string,JSONPath=`.spec.alias` 上述代码中， 分别展示了 .status.replicas, .spec.image, .metadata.uid 以及一个 **不存在的 .spec.alias\n展示字段有三个属性:\n name: 名字， 对外展示的名字，可以与实际属性名不一致。 type: 类型， 字段的属性， 例如 string, bool, integer 等。 JSONPath: 属性路径。 可以通过 kg redis \u0026lt;name\u0026gt; -o json 查看属性实际路径。   注意: 展示属性可以是资源对象中的任意属性， 不一定非的是 Status\n 编译发布，并测试\nkg redis NAME REPLICAS IMAGENAME ALIAS my-op-redis 2 redis:5-alpine  注意: 展示字段的注解代码 //+kubebuilder:priintcolumn 必须贴合 //+kubebuilder:subresource:status。中间不能有空格， 上下都行， 还是建写在 status 的下方。\n 非发布状态确认 #  在不发布的情况下， 使用 make install 生成最新部署配置文件\n在 /config/crd/bases/myapp.tangx.in_redis.yaml 可以查看\nspec: versions: - additionalPrinterColumns: - jsonPath: .status.replicas name: Replicas type: integer - jsonPath: .spec.image name: ImageName type: string - jsonPath: .spec.alias name: Alias type: string # ... "},{"id":14,"href":"/kubebuilder-zero-to-one/14-kubectl-scale-autoscale-support/","title":"14 Kubectl Scale Autoscale Support","section":"","content":"支持 kubectl scale 和 kubectl autoscale 命令 #  在 k8s 自定义资源中有关于 scale 和 hpa 的 subresources 字段， 只有这些字段被定义的时候才能支持 scale 和 autoscale 命令\n官方定义如下\n https://kubernetes.io/zh/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#scale-subresource\n 在 kubebuilde 中， 使用 //+kubebuilder:subresource:scale 增加注解， 生成对应的配置。\n注意， 未知需要在 //+kubebuilder:subresource:status 下方\n//+kubebuilder:object:root=true //+kubebuilder:subresource:status //+kubebuilder:subresource:scale:specpath=.spec.replicas,statuspath=.status.replicas,selectorpath=.status.selector 三个关键字段:\n  specpath: specReplicasPath 指定定制资源内与 scale.spec.replicas 对应的 JSON 路径。\n 此字段为 必需值 。 只可以使用 .spec 下的 JSON 路径，只可使用带句点的路径。 如果定制资源的 specReplicasPath 下没有取值，则针对 /scale 子资源执行 GET 操作时会返回错误。    statuspath: statusReplicasPath 指定定制资源内与 scale.status.replicas 对应的 JSON 路径。\n 此字段为 必需值 。 只可以使用 .status 下的 JSON 路径，只可使用带句点的路径。 如果定制资源的 statusReplicasPath 下没有取值，则针对 /scale 子资源的 副本个数状态值默认为 0。    selectorpath: labelSelectorPath 指定定制资源内与 scale.status.selector 对应的 JSON 路径。\n 此字段为可选值。 此字段必须设置才能使用 HPA 。 只可以使用 .status 或 .spec 下的 JSON 路径，只可使用带句点的路径。 如果定制资源的 labelSelectorPath 下没有取值，则针对 /scale 子资源的 选择算符状态值默认为空字符串。 此 JSON 路径所指向的字段必须是一个字符串字段（而不是复合的选择算符结构）， 其中包含标签选择算符串行化的字符串形式。    使用之后 make install 编译之后, 可以在 subresources 下找到响应字段。\n# config/crd/xxx.yml --- apiVersion: apiextensions.k8s.io/v1 kind: CustomResourceDefinition metadata: annotations: controller-gen.kubebuilder.io/version: v0.7.0 creationTimestamp: null name: redis.myapp.tangx.in spec: # ... 省略 subresources: scale: labelSelectorPath: .spec.selector specReplicasPath: .spec.replicas statusReplicasPath: .status.replicas kube scale #  使用 kubectl scale 命令进行扩缩容\n# k scale --replicas=2 redis/my-op-redis redis.myapp.tangx.in/my-op-redis scaled # kgp NAME READY STATUS RESTARTS AGE my-op-redis-0 1/1 Running 0 29m my-op-redis-1 1/1 Running 0 29m kube autoscale #  # k autoscale redis my-op-redis --min=3 --max=10 horizontalpodautoscaler.autoscaling/my-op-redis autoscaled # kgp NAME READY STATUS RESTARTS AGE my-op-redis-0 1/1 Running 0 29m my-op-redis-1 1/1 Running 0 29m my-op-redis-2 1/1 Running 0 4s "},{"id":15,"href":"/kubebuilder-zero-to-one/index.html","title":"Index","section":"","content":"kubebuilder 从零开始\n  1. 使用 kuberbuilder 初始化项目  2. 简单跑一跑  3. 发布 crd controller  4. 使用注解完整字段值约束  5. 通过 webhook 进行字段验证  6. 使用 Operator 创建并发布一个 Pod 7. K8S 父子资源删除管理   7.1. 使用 OwnerReference 管理 redis operator 创建的 Pod  7.2. 使用 finalizers 管理 redis operator 创建的 Pod    9. Pod 扩容与缩容  10. 监听 k8s 事件  11. 重建被删除的 Pod  12. 使用 controllerutil 优化代码  13. 增加 event 事件支持  14. 添加 Status 状态字段  "},{"id":16,"href":"/kubebuilder-zero-to-one/readme/","title":"Readme","section":"","content":"kubebuilder 从 0 到 1 #  从 0 开始写一个 Redis Operator\n"},{"id":17,"href":"/kubebuilder-zero-to-one/summary/","title":"Summary","section":"","content":"kubebuilder 从零开始\n  1. 使用 kuberbuilder 初始化项目  2. 简单跑一跑  3. 发布 crd controller  4. 使用注解完整字段值约束  5. 通过 webhook 进行字段验证  6. 使用 Operator 创建并发布一个 Pod 7. K8S 父子资源删除管理   7.1. 使用 OwnerReference 管理 redis operator 创建的 Pod  7.2. 使用 finalizers 管理 redis operator 创建的 Pod    9. Pod 扩容与缩容  10. 监听 k8s 事件  11. 重建被删除的 Pod  12. 使用 controllerutil 优化代码  13. 增加 event 事件支持  14. 添加 Status 状态字段  "}]